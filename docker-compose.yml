x-build-args:
  &cu126_args
  PIP_EXTRA_INDEX_URL: https://download.pytorch.org/whl/cu126

services:
  film2_bootstrap:
    image: python:3.11-slim
    container_name: film2_bootstrap
    command: ["/bin/bash","-lc","chmod +x /bootstrap/bootstrap_models.sh && /bootstrap/bootstrap_models.sh"]
    environment:
      - FILM2_MODELS=/opt/models
      - HF_HOME=/opt/models/.hf
      - TRANSFORMERS_CACHE=/opt/models/.hf
      - TORCH_HOME=/opt/models/.torch
    volumes:
      - film2_models:/opt/models
      - ./bootstrap:/bootstrap:ro
    restart: "no"
  nvidia_init:
    image: ubuntu:22.04
    container_name: nvidia_init
    privileged: true
    pid: "host"
    network_mode: host
    restart: "no"
    environment:
      - TARGET_LIB_DIR=/host/usr/lib/x86_64-linux-gnu
      - HOST_ROOT=/host
    volumes:
      - /:/host:rw
      - /proc:/host/proc:rw
      - /sys:/host/sys:rw
      - /dev:/host/dev:rw
      - /run/systemd/resolve:/host/run/systemd/resolve:ro
      - ./docker/nvidia-init/init.sh:/init.sh:ro
    entrypoint: ["/bin/bash","-lc","cp /init.sh /tmp/init.sh; chmod +x /tmp/init.sh; sed -i 's/\\r$//' /tmp/init.sh || true; bash /tmp/init.sh || true"]

  


  pgvector:
    image: pgvector/pgvector:0.7.4-pg16
    container_name: pgvector_db
    restart: unless-stopped
    environment:
      - POSTGRES_DB=ragdb
      - POSTGRES_USER=rag
      - POSTGRES_PASSWORD=ragpass
    ports:
      - "5432:5432"
    volumes:
      - pgvector_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U rag -d ragdb"]
      interval: 10s
      timeout: 5s
      retries: 12
  ollama_qwen:
    image: ollama/ollama:latest
    container_name: ollama_qwen
    restart: unless-stopped
    runtime: nvidia
    devices:
      - "nvidia.com/gpu=all"
    depends_on:
      nvidia_init:
        condition: service_completed_successfully
    ports:
      - "11434:11434"
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_KEEP_ALIVE=24h
      - NVIDIA_VISIBLE_DEVICES=${OLLAMA_QWEN_GPU:-all}
    volumes:
      - ollama_qwen_data:/root/.ollama
    healthcheck:
      test: ["CMD", "sh", "-lc", "ollama list >/dev/null 2>&1"]
      interval: 10s
      timeout: 5s
      retries: 12

  init_qwen:
    image: curlimages/curl:8.7.1
    container_name: init_qwen
    depends_on:
      ollama_qwen:
        condition: service_started
    environment:
      - QWEN_MODEL_ID=${QWEN_MODEL_ID:-qwen3:32b-instruct-q4_K_M}
    command: ["sh","-lc","until curl -s http://ollama_qwen:11434/api/tags >/dev/null; do sleep 2; done; curl -s -X POST http://ollama_qwen:11434/api/pull -H 'Content-Type: application/json' -d '{\"name\":\"${QWEN_MODEL_ID}\"}'"]

  ollama_gptoss:
    image: ollama/ollama:latest
    container_name: ollama_gptoss
    restart: unless-stopped
    runtime: nvidia
    devices:
      - "nvidia.com/gpu=all"
    depends_on:
      nvidia_init:
        condition: service_completed_successfully
    ports:
      - "11435:11434"
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_KEEP_ALIVE=24h
      - NVIDIA_VISIBLE_DEVICES=${OLLAMA_GPTOSS_GPU:-all}
    volumes:
      - ollama_gptoss_data:/root/.ollama
    healthcheck:
      test: ["CMD", "sh", "-lc", "ollama list >/dev/null 2>&1"]
      interval: 10s
      timeout: 5s
      retries: 12

  init_gptoss:
    image: curlimages/curl:8.7.1
    container_name: init_gptoss
    depends_on:
      ollama_gptoss:
        condition: service_started
    environment:
      - GPTOSS_MODEL_ID=${GPTOSS_MODEL_ID}
    command: ["sh","-lc","until curl -s http://ollama_gptoss:11434/api/tags >/dev/null; do sleep 2; done; curl -s -X POST http://ollama_gptoss:11434/api/pull -H 'Content-Type: application/json' -d '{\"name\":\"${GPTOSS_MODEL_ID}\"}'"]

  orchestrator:
    build:
      context: ./orchestrator
      dockerfile: Dockerfile
      network: host
      args:
        <<: *cu126_args
    container_name: void_orchestrator
    restart: unless-stopped
    depends_on:
      pgvector:
        condition: service_healthy
      ollama_qwen:
        condition: service_healthy
      ollama_gptoss:
        condition: service_healthy
      yue:
        condition: service_healthy
      sao:
        condition: service_healthy
      demucs:
        condition: service_healthy
      xtts:
        condition: service_healthy
      film2_bootstrap:
        condition: service_completed_successfully
    environment:
      - FILM2_MODELS=/opt/models
      - FILM2_DATA=/srv/film2
      - QWEN_BASE_URL=http://ollama_qwen:11434
      - QWEN_MODEL_ID=${QWEN_MODEL_ID:-qwen3:32b-instruct-q4_K_M}
      - GPTOSS_BASE_URL=http://ollama_gptoss:11434
      - GPTOSS_MODEL_ID=${GPTOSS_MODEL_ID}
      # Enable ICW + Committee + Tools by default
      - ICW_MODE=${ICW_MODE:-committee}
      - ICW_DISABLE=0
      - AUTO_EXECUTE_TOOLS=${AUTO_EXECUTE_TOOLS:-true}
      - ALLOW_TOOL_EXECUTION=${ALLOW_TOOL_EXECUTION:-true}
      - ENABLE_DEBATE=${ENABLE_DEBATE:-true}
      - MAX_DEBATE_TURNS=${MAX_DEBATE_TURNS:-1}
      - ABLATE=${ABLATE:-on}
      - ABLATE_EXPORT=${ABLATE_EXPORT:-on}
      - ABLATE_SCOPE=${ABLATE_SCOPE:-auto}
      - RAG_CACHE_TTL_SEC=${RAG_CACHE_TTL_SEC:-300}
      - DEFAULT_NUM_CTX=${DEFAULT_NUM_CTX:-4096}
      - DEFAULT_TEMPERATURE=${DEFAULT_TEMPERATURE:-0.3}
      - ENABLE_WEBSEARCH=${ENABLE_WEBSEARCH:-true}
      - DEFAULT_NUM_CTX=${DEFAULT_NUM_CTX:-4096}
      - DEFAULT_TEMPERATURE=${DEFAULT_TEMPERATURE:-0.3}
      - ENABLE_WEBSEARCH=${ENABLE_WEBSEARCH:-true}
      - SERPAPI_API_KEY
      - EXECUTOR_BASE_URL=http://executor:8081
      - PLANNER_MODEL=${PLANNER_MODEL:-qwen3}
      - ENABLE_DEBATE=${ENABLE_DEBATE:-true}
      - MAX_DEBATE_TURNS=${MAX_DEBATE_TURNS:-1}
      - MCP_HTTP_BRIDGE_URL
      - N8N_WEBHOOK_URL=${N8N_WEBHOOK_URL:-}
      - ASSEMBLER_API_URL=http://assembler:9095
      - ICW_API_URL=http://icw:8085
      - TEACHER_API_URL=http://teacher:8097
      - AUTO_EXECUTE_TOOLS=${AUTO_EXECUTE_TOOLS:-true}
      - POSTGRES_HOST=pgvector
      - POSTGRES_PORT=5432
      - POSTGRES_DB=ragdb
      - POSTGRES_USER=rag
      - POSTGRES_PASSWORD=ragpass
      - EMBEDDING_MODEL_NAME=${EMBEDDING_MODEL_NAME:-BAAI/bge-large-en-v1.5}
      - RAG_CACHE_TTL_SEC=${RAG_CACHE_TTL_SEC:-300}
      - XTTS_API_URL=http://xtts:8020
      - WHISPER_API_URL=http://whisper:9090
      - FACEID_API_URL=http://faceid:7000
      - MUSIC_API_URL=http://music:7860
      - YUE_API_URL=http://yue:9001
      - SAO_API_URL=http://sao:9002
      - DEMUCS_API_URL=http://demucs:9101
      - COMFYUI_API_URL=${COMFYUI_API_URL:-http://comfyui:8188}
      - COMFYUI_API_URLS=${COMFYUI_API_URLS:-}
      - COMFYUI_REPLICAS=${COMFYUI_REPLICAS:-1}
      - SCENE_SUBMIT_CONCURRENCY=${SCENE_SUBMIT_CONCURRENCY:-4}
      - PUBLIC_BASE_URL=${PUBLIC_BASE_URL:-}
      - UPLOAD_DIR=/workspace/uploads
      - VLM_API_URL=http://vlm:8050
      - OCR_API_URL=http://ocr:8070
    ports:
      - "8000:8000"
    volumes:
      - ./:/workspace
      - film2_models:/opt/models:ro
      - film2_data:/srv/film2

  xtts:
    build:
      context: ./services/xtts
      dockerfile: Dockerfile
      network: host
      args:
        <<: *cu126_args
    container_name: xtts_service
    restart: unless-stopped
    runtime: nvidia
    devices:
      - "nvidia.com/gpu=all"
    depends_on:
      nvidia_init:
        condition: service_completed_successfully
    environment:
      - XTTS_MODEL_NAME=tts_models/multilingual/multi-dataset/xtts_v2
      - NVIDIA_VISIBLE_DEVICES=all
      - PIP_EXTRA_INDEX_URL=https://download.pytorch.org/whl/cu126
      - HF_HOME=/models/hf
      - TRANSFORMERS_CACHE=/models/hf
      - TORCH_HOME=/models/torch
    expose:
      - "8020"
    ports:
      - "8020:8020"
    shm_size: "1g"
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:8020/healthz"]
      interval: 10s
      timeout: 5s
      retries: 12
    volumes:
      - hf_cache:/models/hf
      - torch_cache:/models/torch
      - tts_cache:/root/.local/share/tts
      - film2_models:/opt/models:ro
      - film2_data:/srv/film2
    

  whisper:
    build:
      context: ./services/whisper
      dockerfile: Dockerfile
      network: host
    container_name: whisper_service
    restart: unless-stopped
    runtime: nvidia
    devices:
      - "nvidia.com/gpu=all"
    depends_on:
      nvidia_init:
        condition: service_completed_successfully
    environment:
      - WHISPER_MODEL_NAME=large-v3
      - WHISPER_CPU_THREADS=8
      - NVIDIA_VISIBLE_DEVICES=all
    expose:
      - "9090"
    ports:
      - "9090:9090"
    

  faceid:
    build:
      context: ./services/faceid
      dockerfile: Dockerfile
      network: host
    container_name: faceid_service
    restart: unless-stopped
    runtime: nvidia
    devices:
      - "nvidia.com/gpu=all"
    depends_on:
      nvidia_init:
        condition: service_completed_successfully
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
    expose:
      - "7000"
    ports:
      - "7000:7000"
    

  comfyui:
    build:
      context: ./services/comfyui
      dockerfile: Dockerfile
      network: host
      args:
        <<: *cu126_args
    container_name: comfyui
    restart: unless-stopped
    runtime: nvidia
    depends_on:
      comfyui_init:
        condition: service_completed_successfully
      nvidia_init:
        condition: service_completed_successfully
    devices:
      - "nvidia.com/gpu=all"
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - PIP_EXTRA_INDEX_URL=https://download.pytorch.org/whl/cu126
    expose:
      - "8188"
    ports:
      - "8188:8188"
    shm_size: "1g"
    volumes:
      - comfyui_models:/comfyui/models
      - comfyui_custom_nodes:/comfyui/custom_nodes
    # For GPU deployments, create additional comfyui-* services and set COMFYUI_API_URLS accordingly

  # Optional additional ComfyUI replicas for dynamic distribution
  # After enabling, set COMFYUI_API_URLS to include all endpoints, e.g.:
  # COMFYUI_API_URLS=http://comfyui:8188,http://comfyui-1:8188,http://comfyui-2:8188
  # And access from host via the mapped ports below for debugging
  comfyui-1:
    build:
      context: ./services/comfyui
      dockerfile: Dockerfile
      network: host
      args:
        <<: *cu126_args
    container_name: comfyui-1
    profiles: ["comfy-replicas"]
    restart: unless-stopped
    runtime: nvidia
    depends_on:
      comfyui_init:
        condition: service_completed_successfully
      nvidia_init:
        condition: service_completed_successfully
    devices:
      - "nvidia.com/gpu=all"
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
    expose:
      - "8188"
    ports:
      - "8189:8188"
    volumes:
      - comfyui_models:/comfyui/models
      - comfyui_custom_nodes:/comfyui/custom_nodes

  comfyui-2:
    build:
      context: ./services/comfyui
      dockerfile: Dockerfile
      network: host
      args:
        <<: *cu126_args
    container_name: comfyui-2
    profiles: ["comfy-replicas"]
    restart: unless-stopped
    runtime: nvidia
    depends_on:
      comfyui_init:
        condition: service_completed_successfully
      nvidia_init:
        condition: service_completed_successfully
    devices:
      - "nvidia.com/gpu=all"
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
    expose:
      - "8188"
    ports:
      - "8190:8188"
    volumes:
      - comfyui_models:/comfyui/models
      - comfyui_custom_nodes:/comfyui/custom_nodes

  comfyui_init:
    build:
      context: ./services/comfyui_init
      dockerfile: Dockerfile
      network: host
    container_name: comfyui_init
    environment:
      - HUGGINGFACE_TOKEN=${HUGGINGFACE_TOKEN:-}
      - COMFYUI_REPLICAS=${COMFYUI_REPLICAS:-1}
      - COMFYUI_GPU_DEVICES=${COMFYUI_GPU_DEVICES:-}
    volumes:
      - comfyui_models:/comfyui/models
      - comfyui_custom_nodes:/comfyui/custom_nodes
    command: ["python", "/app/init_assets.py"]

  n8n:
    image: n8nio/n8n:latest
    container_name: n8n
    restart: unless-stopped
    environment:
      - N8N_HOST=n8n
      - N8N_PROTOCOL=http
      - N8N_PORT=5678
      - N8N_USER_MANAGEMENT_DISABLED=true
      - N8N_DIAGNOSTICS_ENABLED=false
      - N8N_VERSION_NOTIFICATIONS_ENABLED=false
      - NODE_ENV=production
      # Recommended settings to silence warnings and enable runners
      - N8N_ENFORCE_SETTINGS_FILE_PERMISSIONS=true
      - DB_SQLITE_POOL_SIZE=5
      - N8N_RUNNERS_ENABLED=true
      - N8N_GIT_NODE_DISABLE_BARE_REPOS=true
    expose:
      - "5678"
    ports:
      - "5678:5678"
    volumes:
      - n8n_data:/home/node/.n8n
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:5678/healthz"]
      interval: 10s
      timeout: 5s
      retries: 12

  n8n_init:
    image: n8nio/n8n:latest
    container_name: n8n_init
    depends_on:
      n8n:
        condition: service_started
    environment:
      - N8N_HOST=n8n
      - N8N_PROTOCOL=http
      - N8N_PORT=5678
      - N8N_USER_MANAGEMENT_DISABLED=true
      - NODE_ENV=production
    volumes:
      - n8n_data:/home/node/.n8n
      - ./services/n8n/workflows:/workflows:ro
    command: sh -lc "n8n import:workflow --input /workflows/film_assemble.json --separate"

  music:
    build:
      context: ./services/music
      dockerfile: Dockerfile
      network: host
      args:
        <<: *cu126_args
    container_name: music_service
    restart: unless-stopped
    runtime: nvidia
    devices:
      - "nvidia.com/gpu=all"
    depends_on:
      nvidia_init:
        condition: service_completed_successfully
    environment:
      - MUSIC_MODEL_ID=facebook/musicgen-small
      - NVIDIA_VISIBLE_DEVICES=all
      - PIP_EXTRA_INDEX_URL=https://download.pytorch.org/whl/cu126
      - HF_HOME=/models/hf
      - TRANSFORMERS_CACHE=/models/hf
      - TORCH_HOME=/models/torch
    expose:
      - "7860"
    ports:
      - "7860:7860"
    shm_size: "1g"
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:7860/healthz"]
      interval: 10s
      timeout: 5s
      retries: 12
    volumes:
      - hf_cache:/models/hf
      - torch_cache:/models/torch
      - film2_models:/opt/models:ro
      - film2_data:/srv/film2
    
  yue:
    build:
      context: ./services/yue
      dockerfile: Dockerfile
      network: host
      args:
        <<: *cu126_args
    container_name: yue_service
    restart: unless-stopped
    runtime: nvidia
    devices:
      - "nvidia.com/gpu=all"
    depends_on:
      nvidia_init:
        condition: service_completed_successfully
    environment:
      - YUE_MODEL_ID=${YUE_MODEL_ID:-facebook/musicgen-large}
      - NVIDIA_VISIBLE_DEVICES=all
      - PIP_EXTRA_INDEX_URL=https://download.pytorch.org/whl/cu126
      - HF_HOME=/models/hf
      - TRANSFORMERS_CACHE=/models/hf
      - TORCH_HOME=/models/torch
    expose:
      - "9001"
    ports:
      - "9001:9001"
    shm_size: "1g"
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:9001/healthz"]
      interval: 10s
      timeout: 5s
      retries: 12
    volumes:
      - hf_cache:/models/hf
      - torch_cache:/models/torch
      - film2_models:/opt/models:ro
      - film2_data:/srv/film2

  demucs:
    build:
      context: ./services/demucs
      dockerfile: Dockerfile
      network: host
      args:
        <<: *cu126_args
    container_name: demucs_service
    restart: unless-stopped
    environment:
      - PIP_EXTRA_INDEX_URL=https://download.pytorch.org/whl/cu126
      - TORCH_HOME=/models/torch
    expose:
      - "9101"
    ports:
      - "9101:9101"
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:9101/healthz"]
      interval: 10s
      timeout: 5s
      retries: 12
    volumes:
      - torch_cache:/models/torch
      - demucs_cache:/root/.cache/demucs
      - film2_models:/opt/models:ro
      - film2_data:/srv/film2

  sao:
    build:
      context: ./services/sao
      dockerfile: Dockerfile
      network: host
    container_name: sao_service
    restart: unless-stopped
    runtime: nvidia
    devices:
      - "nvidia.com/gpu=all"
    depends_on:
      nvidia_init:
        condition: service_completed_successfully
    environment:
      - SAO_MODEL_ID=${SAO_MODEL_ID:-facebook/musicgen-large}
      - NVIDIA_VISIBLE_DEVICES=all
      - PIP_EXTRA_INDEX_URL=https://download.pytorch.org/whl/cu126
      - HF_HOME=/models/hf
      - TRANSFORMERS_CACHE=/models/hf
      - TORCH_HOME=/models/torch
    expose:
      - "9002"
    ports:
      - "9002:9002"
    shm_size: "1g"
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:9002/healthz"]
      interval: 10s
      timeout: 5s
      retries: 12
    volumes:
      - hf_cache:/models/hf
      - torch_cache:/models/torch
      - film2_models:/opt/models:ro
      - film2_data:/srv/film2

  vlm:
    build:
      context: ./services/vlm
      dockerfile: Dockerfile
      network: host
      args:
        <<: *cu126_args
    container_name: vlm_service
    restart: unless-stopped
    runtime: nvidia
    devices:
      - "nvidia.com/gpu=all"
    depends_on:
      nvidia_init:
        condition: service_completed_successfully
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - PIP_EXTRA_INDEX_URL=https://download.pytorch.org/whl/cu126
      - HF_HOME=/models/hf
      - TRANSFORMERS_CACHE=/models/hf
      - TORCH_HOME=/models/torch
    expose:
      - "8050"
    ports:
      - "8050:8050"
    shm_size: "1g"
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:8050/healthz"]
      interval: 10s
      timeout: 5s
      retries: 12
    volumes:
      - hf_cache:/models/hf
      - torch_cache:/models/torch
      - film2_models:/opt/models:ro
      - film2_data:/srv/film2
    

  ocr:
    build:
      context: ./services/ocr
      dockerfile: Dockerfile
      network: host
    container_name: ocr_service
    restart: unless-stopped
    expose:
      - "8070"
    ports:
      - "8070:8070"

  executor:
    depends_on:
      orchestrator:
        condition: service_started
    build:
      context: ./executor
      dockerfile: Dockerfile
      network: host
    container_name: void_executor
    restart: unless-stopped
    environment:
      - WORKSPACE_DIR=/workspace
      - EXEC_TIMEOUT_SEC=${EXEC_TIMEOUT_SEC:-30}
      - EXEC_MEMORY_MB=${EXEC_MEMORY_MB:-2048}
      - ALLOW_SHELL=${ALLOW_SHELL:-false}
      - SHELL_WHITELIST=${SHELL_WHITELIST:-}
    expose:
      - "8081"
    ports:
      - "8081:8081"
    volumes:
      - ./:/workspace

  drt:
    build:
      context: ./services/drt
      dockerfile: Dockerfile
      network: host
    container_name: deep_research_tool
    restart: unless-stopped
    environment:
      - PUBLIC_BASE_URL=${PUBLIC_BASE_URL:-}
    expose:
      - "8086"
    ports:
      - "8086:8086"
    volumes:
      - ./:/workspace

  film2:
    build:
      context: ./services/film2
      dockerfile: Dockerfile
      network: host
    container_name: film2_service
    restart: unless-stopped
    runtime: nvidia
    devices:
      - "nvidia.com/gpu=all"
    depends_on:
      nvidia_init:
        condition: service_completed_successfully
    environment:
      - PUBLIC_BASE_URL=${PUBLIC_BASE_URL:-}
      - NVIDIA_VISIBLE_DEVICES=all
    expose:
      - "8090"
    ports:
      - "8090:8090"
    volumes:
      - ./:/workspace

  ablation:
    build:
      context: ./services/ablation
      dockerfile: Dockerfile
      network: host
    container_name: ablation_layer
    restart: unless-stopped
    environment:
      - PUBLIC_BASE_URL=${PUBLIC_BASE_URL:-}
      - ABLCODER_URL=${ABLCODER_URL:-}
    expose:
      - "8095"
    ports:
      - "8095:8095"
    volumes:
      - ./:/workspace

  teacher:
    build:
      context: ./services/teacher
      dockerfile: Dockerfile
      network: host
    container_name: teacher_service
    restart: unless-stopped
    environment:
      - PUBLIC_BASE_URL=${PUBLIC_BASE_URL:-}
    expose:
      - "8097"
    ports:
      - "8097:8097"
    volumes:
      - ./:/workspace

  chatui:
    build:
      context: ./services/chatui
      dockerfile: Dockerfile
      network: host
    container_name: chatui
    restart: unless-stopped
    environment:
      - ORCHESTRATOR_URL=http://orchestrator:8000
      - POSTGRES_HOST=pgvector
      - POSTGRES_PORT=5432
      - POSTGRES_DB=ragdb
      - POSTGRES_USER=rag
      - POSTGRES_PASSWORD=ragpass
    depends_on:
      orchestrator:
        condition: service_started
      pgvector:
        condition: service_healthy
    ports:
      - "3000:3000"

volumes:
  ollama_qwen_data: {}
  ollama_gptoss_data: {}
  pgvector_data: {}
  comfyui_models: {}
  comfyui_custom_nodes: {}
  n8n_data: {}
  hf_cache: {}
  torch_cache: {}
  tts_cache: {}
  demucs_cache: {}
  film2_models: {}
  film2_data: {}


